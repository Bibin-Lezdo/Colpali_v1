{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers pdf2image icecream sentencepiece protobuf flash_attn einops accelerate\n",
    "!apt-get install -y poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade byaldi\n",
    "%pip install  pdf2image\n",
    "%pip install qwen-vl-utils\n",
    "# %pip install huggingface_hub\n",
    "# %pip install pdf2image\n",
    "!apt-get install -y poppler-utils\n",
    "%pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "from pdf2image import convert_from_path\n",
    "from byaldi import RAGMultiModalModel\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# notebook_login()\n",
    "file_path='/home/Redacted.pdf'\n",
    "images = convert_from_path(file_path)\n",
    "\n",
    "\n",
    "login(token='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')\n",
    "RAG = RAGMultiModalModel.from_pretrained(\"vidore/colpali\")\n",
    "\n",
    "\n",
    "\n",
    "RAG.index(\n",
    "    input_path=file_path,\n",
    "    index_name=\"image_index\",\n",
    "    store_collection_with_index=False,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byaldi import RAGMultiModalModel\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor,MllamaForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "text_query=\"What was the value of the blood pressure and heart rate on 21/10/22 at 15:00?\"\n",
    "\n",
    "def get_results(text_query):\n",
    "    text_query = text_query\n",
    "    image_limit=1\n",
    "    results = RAG.search(text_query,k=image_limit)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results\n",
    "    return results\n",
    "\n",
    "results = get_results(text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)\n",
    "def get_image_list(results):\n",
    "     image_index = results[0][\"page_num\"] - 1\n",
    "\n",
    "     image = images[image_index]\n",
    "\n",
    "     extracted_images = []  \n",
    "     for result in results:\n",
    "          extracted_images.append(images[result['page_num']-1])\n",
    "     return extracted_images\n",
    "     \n",
    "extracted_images=get_image_list(results)\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from icecream import ic\n",
    "import time\n",
    "\n",
    "class DocOwlInfer():\n",
    "    def __init__(self, ckpt_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_path, use_fast=False)\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(ckpt_path, \n",
    "            trust_remote_code=True, \n",
    "            low_cpu_mem_usage=True, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto',\n",
    "            offload_buffers=True)\n",
    "\n",
    "        self.model.init_processor(tokenizer=self.tokenizer, basic_image_size=504, crop_anchors='grid_12')\n",
    "        \n",
    "    def inference(self, images, query):\n",
    "        messages = [{'role': 'USER', 'content': '<|image|>'*len(images)+query}]\n",
    "        answer = self.model.chat(messages=messages, images=images, tokenizer=self.tokenizer)\n",
    "        return answer\n",
    "\n",
    "\n",
    "docowl = DocOwlInfer(ckpt_path='mPLUG/DocOwl2')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer = docowl.inference(extracted_images, query='Extract relevant information from the following pages and provide it as JSON structured data. Include only the specified fields.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"gpt2\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "def get_answer_from_json(json_data, question):\n",
    "    # prompt = f\"Here is some data in JSON format:\\n{json_data}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # output = model.generate(\n",
    "    #     inputs.input_ids,\n",
    "    #     # max_length=300,\n",
    "    #     max_new_tokens=300,\n",
    "    #     num_return_sequences=1,\n",
    "    #     temperature=0.7,  \n",
    "    #     top_p=0.9,       \n",
    "    #     do_sample=True\n",
    "    # )\n",
    "\n",
    "    # answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # answer = answer.split(\"Answer:\")[1].strip() if \"Answer:\" in answer else answer.strip()\n",
    "\n",
    "\n",
    "    login(token='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')\n",
    "\n",
    "    model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\", \n",
    "        model=model_id, \n",
    "        torch_dtype=torch.bfloat16, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    sections_text = \"\"\n",
    "    for section in json_data[\"sections\"]:\n",
    "        sections_text += f\"\\n{section['title']} {section['text']}\"\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"Answer the question only from the following JSON data. \n",
    "    If the answer is not provided in the JSON data, respond with 'Not provided in JSON.'\n",
    "\n",
    "    JSON data:\n",
    "    {sections_text}\n",
    "\n",
    "    Question: {question}\"\"\"\n",
    "\n",
    "    answer = pipe(prompt,max_new_tokens=300)\n",
    "    print('final ans')\n",
    "    print(answer[0]['generated_text'])\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query=input('Enter query')\n",
    "results=get_results(text_query)\n",
    "print(results)\n",
    "extracted_images=get_image_list(results)\n",
    "answer = docowl.inference(extracted_images, query='Extract all information from the following pages and provide it as JSON structured data. Include only the specified fields.')\n",
    "get_answer_from_json(answer, text_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdf2image import convert_from_path\n",
    "\n",
    "# pdf_path = '/home/Redacted.pdf'  \n",
    "# images = convert_from_path(pdf_path)\n",
    "# def limit_images(limit):\n",
    "#     limited_images=[]\n",
    "#     for i in range(0,len(images)-1):\n",
    "#         if(i<limit):\n",
    "#             limited_images.append(images[i])\n",
    "#         else:\n",
    "#             break\n",
    "#     return limited_images\n",
    "\n",
    "# images = limit_images(3)\n",
    "# print(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
